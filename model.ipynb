{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BnfHWWfdsgHS"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymorphy2\n",
    "import os\n",
    "import string\n",
    "import tempfile\n",
    "import tensorflow as tf\n",
    "\n",
    "from gensim.utils import tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "from tensorboard import summary as summary_lib\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TJ5Jop58sshc"
   },
   "outputs": [],
   "source": [
    "esenin = pd.read_csv('esenin.csv')\n",
    "random = pd.read_csv('random.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "id": "hYaL6XoAFWTy",
    "outputId": "1e88b1b1-bb5b-4718-a3b5-54718408066f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>poem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Есть музыка, стихи и танцы,\\r\\r\\nЕсть ложь и л...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Радость, как плотвица быстрая,\\r\\r\\nЮрко свети...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Алый мрак в небесной черни\\r\\r\\nНачертил пожар...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Сестре Шуре\\r\\r\\n\\r\\r\\nАх, как мн...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Ах, метель такая, просто черт возьми!\\r\\r\\nЗаб...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               poem\n",
       "0   0  Есть музыка, стихи и танцы,\\r\\r\\nЕсть ложь и л...\n",
       "1   1  Радость, как плотвица быстрая,\\r\\r\\nЮрко свети...\n",
       "2   2  Алый мрак в небесной черни\\r\\r\\nНачертил пожар...\n",
       "3   3               Сестре Шуре\\r\\r\\n\\r\\r\\nАх, как мн...\n",
       "4   4  Ах, метель такая, просто черт возьми!\\r\\r\\nЗаб..."
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "esenin.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kLVHLzFYBH1v"
   },
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X4QsRdVSsgHi"
   },
   "outputs": [],
   "source": [
    "vocab_size = 5000\n",
    "sentence_size = 200\n",
    "embedding_size = 50\n",
    "pad_id = 0\n",
    "\n",
    "stops = stopwords.words('russian')\n",
    "stops.extend(list(string.punctuation))\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "model_dir = tempfile.mkdtemp()\n",
    "\n",
    "def text_preprocessor(str_input):\n",
    "    '''Dropping stop-words and normalization.'''\n",
    "    output = []\n",
    "    for token in list(tokenize(str_input, lowercase=True)):\n",
    "        if token in stops:\n",
    "            continue\n",
    "        else:\n",
    "            output.append(morph.parse(token)[0].normal_form)\n",
    "    return ' '.join(output)\n",
    "\n",
    "esenin['preprocessed_poem'] = esenin['poem'].apply(text_preprocessor)\n",
    "random['preprocessed_poem'] = random['poem'].apply(text_preprocessor)\n",
    "\n",
    "tf_idf = TfidfVectorizer(stop_words=stops, max_features=vocab_size)\n",
    "tf_idf.fit(esenin['preprocessed_poem'])\n",
    "\n",
    "esenin['numerical_poem'] = esenin['preprocessed_poem'].apply(\n",
    "    lambda x: [tf_idf.vocabulary_[w] if w in tf_idf.vocabulary_ else 4999 for w in x.split()])\n",
    "random['numerical_poem'] = random['preprocessed_poem'].apply(\n",
    "    lambda x: [tf_idf.vocabulary_[w] if w in tf_idf.vocabulary_ else 4999 for w in x.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sJlBik-oGLGS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((esenin, random))\n",
    "df['target'] = [1] * esenin.shape[0] + [0] * random.shape[0]\n",
    "del esenin, random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "id": "KqjXGJYmLFCt",
    "outputId": "27ce302a-aabf-488d-9e0a-50f7689d9d64"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>poem</th>\n",
       "      <th>preprocessed_poem</th>\n",
       "      <th>numerical_poem</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Есть музыка, стихи и танцы,\\r\\r\\nЕсть ложь и л...</td>\n",
       "      <td>музыка стих танец ложь лесть пускай бранить ст...</td>\n",
       "      <td>[1833, 3947, 4133, 1681, 4999, 2881, 133, 3907...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Радость, как плотвица быстрая,\\r\\r\\nЮрко свети...</td>\n",
       "      <td>радость плотвица быстрый юрко светить вода рук...</td>\n",
       "      <td>[2958, 4999, 169, 4962, 3369, 287, 3252, 1823,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Алый мрак в небесной черни\\r\\r\\nНачертил пожар...</td>\n",
       "      <td>алый мрак небесный чернь начертить пожар грань...</td>\n",
       "      <td>[10, 1826, 1904, 4776, 1900, 2336, 4999, 2610,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Сестре Шуре\\r\\r\\n\\r\\r\\nАх, как мн...</td>\n",
       "      <td>сестра шура ах свет кошка мы ты счесть сердце ...</td>\n",
       "      <td>[3482, 4917, 23, 3364, 1527, 4999, 4999, 4088,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Ах, метель такая, просто черт возьми!\\r\\r\\nЗаб...</td>\n",
       "      <td>ах метель такой просто черта взять забивать кр...</td>\n",
       "      <td>[23, 1760, 4999, 2801, 4780, 248, 998, 1577, 5...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               poem  \\\n",
       "0   0  Есть музыка, стихи и танцы,\\r\\r\\nЕсть ложь и л...   \n",
       "1   1  Радость, как плотвица быстрая,\\r\\r\\nЮрко свети...   \n",
       "2   2  Алый мрак в небесной черни\\r\\r\\nНачертил пожар...   \n",
       "3   3               Сестре Шуре\\r\\r\\n\\r\\r\\nАх, как мн...   \n",
       "4   4  Ах, метель такая, просто черт возьми!\\r\\r\\nЗаб...   \n",
       "\n",
       "                                   preprocessed_poem  \\\n",
       "0  музыка стих танец ложь лесть пускай бранить ст...   \n",
       "1  радость плотвица быстрый юрко светить вода рук...   \n",
       "2  алый мрак небесный чернь начертить пожар грань...   \n",
       "3  сестра шура ах свет кошка мы ты счесть сердце ...   \n",
       "4  ах метель такой просто черта взять забивать кр...   \n",
       "\n",
       "                                      numerical_poem  target  \n",
       "0  [1833, 3947, 4133, 1681, 4999, 2881, 133, 3907...       1  \n",
       "1  [2958, 4999, 169, 4962, 3369, 287, 3252, 1823,...       1  \n",
       "2  [10, 1826, 1904, 4776, 1900, 2336, 4999, 2610,...       1  \n",
       "3  [3482, 4917, 23, 3364, 1527, 4999, 4999, 4088,...       1  \n",
       "4  [23, 1760, 4999, 2801, 4780, 248, 998, 1577, 5...       1  "
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yPGIrOSVsgIC"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['numerical_poem'], df['target'], random_state=42, test_size=0.2)\n",
    "\n",
    "x_train = sequence.pad_sequences(X_train,\n",
    "                                 maxlen=sentence_size,\n",
    "                                 truncating='post',\n",
    "                                 padding='post',\n",
    "                                 value=pad_id)\n",
    "x_test = sequence.pad_sequences(X_test, \n",
    "                                maxlen=sentence_size,\n",
    "                                truncating='post',\n",
    "                                padding='post', \n",
    "                                value=pad_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "colab_type": "code",
    "id": "Mf2XGvjFLLmz",
    "outputId": "ac7c42be-db43-4013-c50e-85fe3833957b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1942, 4999, 4999, 1993, 4999, 4999, 4999, 1744, 4999, 4999,  114,\n",
       "       2889, 1284,   46, 3205,   46, 4999, 4999, 4999, 4999,  218, 2640,\n",
       "        864, 2135, 3716, 4999,  561, 3679,  651,  174, 4999, 2091, 4260,\n",
       "       2958, 4999, 3417, 4999, 3465, 3398, 4999, 2745, 4714, 4999, 4999,\n",
       "       1989, 4759, 2228, 2939, 4047,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0], dtype=int32)"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MxPAl14HsgIX"
   },
   "outputs": [],
   "source": [
    "x_len_train = np.array([min(len(x), sentence_size) for x in X_train])\n",
    "x_len_test = np.array([min(len(x), sentence_size) for x in X_test])\n",
    "\n",
    "def parser(x, length, y):\n",
    "    features = {\"num_poem\": x, \"len\": length}\n",
    "    return features, y\n",
    "\n",
    "def train_input_fn():\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x_train, x_len_train, y_train))\n",
    "    dataset = dataset.shuffle(buffer_size=len(X_train))\n",
    "    dataset = dataset.batch(100)\n",
    "    dataset = dataset.map(parser)\n",
    "    dataset = dataset.repeat()\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    return iterator.get_next()\n",
    "\n",
    "def eval_input_fn():\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x_test, x_len_test, y_test))\n",
    "    dataset = dataset.batch(100)\n",
    "    dataset = dataset.map(parser)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    return iterator.get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ok3G184XBTkw"
   },
   "source": [
    "## Building LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2O7tXvZTsgIc"
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate(classifier):\n",
    "    classifier.train(input_fn=train_input_fn, steps=5000)\n",
    "    eval_results = classifier.evaluate(input_fn=eval_input_fn)\n",
    "    predictions = np.array([p['logistic'][0] for p in classifier.predict(input_fn=eval_input_fn)])\n",
    "        \n",
    "    # Reset the graph to be able to reuse name scopes\n",
    "    tf.reset_default_graph() \n",
    "    # Add a PR summary in addition to the summaries that the classifier writes\n",
    "    pr = summary_lib.pr_curve('precision_recall', predictions=predictions, \n",
    "                              labels=y_test.astype(bool), num_thresholds=21)\n",
    "    with tf.Session() as sess:\n",
    "        writer = tf.summary.FileWriter(os.path.join(classifier.model_dir, 'eval'), sess.graph)\n",
    "        writer.add_summary(sess.run(pr), global_step=0)\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2221
    },
    "colab_type": "code",
    "id": "-hkbpRuzsgI4",
    "outputId": "d3112497-22a6-43f1-f78e-f1f455b6a2d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmp_a61vz4_/lstm', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f0d3bf08240>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmp_a61vz4_/lstm/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.703977, step = 0\n",
      "INFO:tensorflow:global_step/sec: 1.37346\n",
      "INFO:tensorflow:loss = 0.053857423, step = 100 (72.819 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.9699\n",
      "INFO:tensorflow:loss = 0.072486244, step = 200 (33.665 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.97601\n",
      "INFO:tensorflow:loss = 0.014226129, step = 300 (33.602 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.924\n",
      "INFO:tensorflow:loss = 0.008062373, step = 400 (34.201 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.97942\n",
      "INFO:tensorflow:loss = 0.00048564083, step = 500 (33.563 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.98431\n",
      "INFO:tensorflow:loss = 0.001979859, step = 600 (33.505 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.99611\n",
      "INFO:tensorflow:loss = 0.04848008, step = 700 (33.381 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.99853\n",
      "INFO:tensorflow:loss = 0.013272693, step = 800 (33.349 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.99203\n",
      "INFO:tensorflow:loss = 0.0074740304, step = 900 (33.423 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.97809\n",
      "INFO:tensorflow:loss = 0.0005364948, step = 1000 (33.578 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.97119\n",
      "INFO:tensorflow:loss = 0.00029571692, step = 1100 (33.653 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.94149\n",
      "INFO:tensorflow:loss = 0.0008213876, step = 1200 (34.001 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.91671\n",
      "INFO:tensorflow:loss = 0.00019357709, step = 1300 (34.285 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.98022\n",
      "INFO:tensorflow:loss = 0.006730258, step = 1400 (33.550 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.98328\n",
      "INFO:tensorflow:loss = 0.007397102, step = 1500 (33.526 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.98978\n",
      "INFO:tensorflow:loss = 0.00011266072, step = 1600 (33.447 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1663 into /tmp/tmp_a61vz4_/lstm/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 2.96197\n",
      "INFO:tensorflow:loss = 0.009687023, step = 1700 (33.760 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.99181\n",
      "INFO:tensorflow:loss = 0.007876056, step = 1800 (33.421 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.98314\n",
      "INFO:tensorflow:loss = 7.361044e-05, step = 1900 (33.522 sec)\n",
      "INFO:tensorflow:global_step/sec: 3.0008\n",
      "INFO:tensorflow:loss = 6.502888e-05, step = 2000 (33.327 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.99776\n",
      "INFO:tensorflow:loss = 5.1186486e-05, step = 2100 (33.355 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.94588\n",
      "INFO:tensorflow:loss = 3.822695e-05, step = 2200 (33.950 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.98453\n",
      "INFO:tensorflow:loss = 4.26735e-05, step = 2300 (33.507 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.98663\n",
      "INFO:tensorflow:loss = 0.010208424, step = 2400 (33.479 sec)\n",
      "INFO:tensorflow:global_step/sec: 3.01837\n",
      "INFO:tensorflow:loss = 0.007327026, step = 2500 (33.133 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.98466\n",
      "INFO:tensorflow:loss = 2.8632563e-05, step = 2600 (33.502 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.99964\n",
      "INFO:tensorflow:loss = 0.007405055, step = 2700 (33.337 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.98126\n",
      "INFO:tensorflow:loss = 0.006673001, step = 2800 (33.547 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.98785\n",
      "INFO:tensorflow:loss = 2.6485615e-05, step = 2900 (33.470 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.98995\n",
      "INFO:tensorflow:loss = 1.6382262e-05, step = 3000 (33.445 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.9755\n",
      "INFO:tensorflow:loss = 1.7326889e-05, step = 3100 (33.605 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.94936\n",
      "INFO:tensorflow:loss = 2.0906424e-05, step = 3200 (33.907 sec)\n",
      "INFO:tensorflow:global_step/sec: 3.00512\n",
      "INFO:tensorflow:loss = 1.743568e-05, step = 3300 (33.275 sec)\n",
      "INFO:tensorflow:global_step/sec: 3.00024\n",
      "INFO:tensorflow:loss = 3.873303e-05, step = 3400 (33.333 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3456 into /tmp/tmp_a61vz4_/lstm/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 2.98065\n",
      "INFO:tensorflow:loss = 0.07780492, step = 3500 (33.545 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.98955\n",
      "INFO:tensorflow:loss = 0.009160139, step = 3600 (33.451 sec)\n",
      "INFO:tensorflow:global_step/sec: 3.00751\n",
      "INFO:tensorflow:loss = 0.0005986996, step = 3700 (33.254 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.99324\n",
      "INFO:tensorflow:loss = 0.01105822, step = 3800 (33.404 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.97875\n",
      "INFO:tensorflow:loss = 0.00031096244, step = 3900 (33.573 sec)\n",
      "INFO:tensorflow:global_step/sec: 3.00179\n",
      "INFO:tensorflow:loss = 0.014130849, step = 4000 (33.315 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.92581\n",
      "INFO:tensorflow:loss = 0.006895135, step = 4100 (34.175 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.94016\n",
      "INFO:tensorflow:loss = 0.00013390018, step = 4200 (34.012 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.93089\n",
      "INFO:tensorflow:loss = 0.000111990434, step = 4300 (34.119 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.9614\n",
      "INFO:tensorflow:loss = 0.00013555597, step = 4400 (33.769 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.9821\n",
      "INFO:tensorflow:loss = 0.0001054411, step = 4500 (33.536 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.98668\n",
      "INFO:tensorflow:loss = 7.47507e-05, step = 4600 (33.481 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.9857\n",
      "INFO:tensorflow:loss = 6.796159e-05, step = 4700 (33.494 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.98427\n",
      "INFO:tensorflow:loss = 6.142655e-05, step = 4800 (33.510 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.98426\n",
      "INFO:tensorflow:loss = 0.0072693336, step = 4900 (33.506 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5000 into /tmp/tmp_a61vz4_/lstm/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 3.8308957e-05.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-09-07-15:04:41\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmp_a61vz4_/lstm/model.ckpt-5000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-09-07-15:04:42\n",
      "INFO:tensorflow:Saving dict for global step 5000: accuracy = 0.82840234, accuracy_baseline = 0.58579886, auc = 0.86168826, auc_precision_recall = 0.8557994, average_loss = 1.1919531, global_step = 5000, label/mean = 0.41420117, loss = 1.2249601, precision = 0.7808219, prediction/mean = 0.43446848, recall = 0.8142857\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 5000: /tmp/tmp_a61vz4_/lstm/model.ckpt-5000\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmp_a61vz4_/lstm/model.ckpt-5000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "head = tf.contrib.estimator.binary_classification_head()\n",
    "\n",
    "def lstm_model_fn(features, labels, mode):    \n",
    "    # [batch_size x sentence_size x embedding_size]\n",
    "    inputs = tf.contrib.layers.embed_sequence(\n",
    "        features['num_poem'], vocab_size, embedding_size,\n",
    "        initializer=tf.random_uniform_initializer(-1.0, 1.0))\n",
    "\n",
    "    # create an LSTM cell of size 100\n",
    "    lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(100)\n",
    "    \n",
    "    # create the complete LSTM\n",
    "    _, final_states = tf.nn.dynamic_rnn(\n",
    "        lstm_cell, inputs, sequence_length=features['len'], dtype=tf.float32)\n",
    "\n",
    "    # get the final hidden states of dimensionality [batch_size x sentence_size]\n",
    "    outputs = final_states.h\n",
    "\n",
    "    logits = tf.layers.dense(inputs=outputs, units=1)\n",
    "\n",
    "    # This will be None when predicting\n",
    "    if labels is not None:\n",
    "        labels = tf.reshape(labels, [-1, 1])\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "    def _train_op_fn(loss):\n",
    "        return optimizer.minimize(\n",
    "            loss=loss,\n",
    "            global_step=tf.train.get_global_step())\n",
    "\n",
    "    return head.create_estimator_spec(\n",
    "        features=features,\n",
    "        labels=labels,\n",
    "        mode=mode,\n",
    "        logits=logits,\n",
    "        train_op_fn=_train_op_fn)\n",
    "\n",
    "\n",
    "lstm_classifier = tf.estimator.Estimator(model_fn=lstm_model_fn,\n",
    "                                         model_dir=os.path.join(model_dir, 'lstm'))\n",
    "train_and_evaluate(lstm_classifier)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "model2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
